# PRE-PROCESSING PARAMETERS
# debug_mode - save pairs as both npy and jpeg
debug_mode: True
# change_detection - if True, uses T1 + T2. if False, use only T2.
change_detection: True 
# two_classes_problem - if True, uses a binary mask. if False, uses a 3-classes mask.
two_classes_problem: False 
# tiles usados para treinamento e validacao
tiles_tr: [1, 3, 5, 7, 8, 10, 11, 13, 14, 16, 18, 20, 4, 6, 19] # 
tiles_val: [2, 9, 12]
# stride is set to 0.5*patch_size
patch_size: 128
overlap: 0.7
save_patches: True # if False, save only image pairs (expected input from pix2pix net)
save_tiles: True # save testing tiles 
save_image_pairs: True
synthetic_input_mode: 1
max_input_samples: 10000 # maximo de itens a serem gerados para entrada do pix2pix treinado
# 0: mascaras originais
# 1: patches sem desmatamento + masks de desmatamento passado -> pixels de desm. passado >>> novo
# 2: patches com desmatamento passado -> aplico dilate e chamo a dilatacao de desmatamento novo
# 3: patches com as 3 classes -> aplico dilate no desmatamento novo
goal_percentage: 3 # minimum percentage when using dilation
# minimum NEW deforestation required per patch (new deforestation - class 1)
# 0: forest, 1: new deforestation, 2: old deforestation
min_percentage: 3
# path to images
root_path: '../Sentinel2/' 
# path to save patches
output_path: '/share_alpha_2/amandalucas/pix2pix/samples_patch_size_128'
# limite utilizado das imagens de entrada -> image is (17729, 9202)
lim_x: 17700 # 1000
lim_y: 9200 # 7000
# tipo de normalizacao utilizada. ps.: o codigo do pix2pix ja faz a normalizacao entre [0,1]
# 0: MinMax(0,255), 1: Standard, 2: MinMax(0,1), 3: MinMax(-1,1)
type_norm: 0
# quais bandas serao utilizadas
channels: [0, 1, 3] 
# 0 - blue *
# 1 - green *
# 2 - red
# 3 - NIR *
# 4 - vegetation red edge (0.705 um)
# 5 - vegetation red edge (0.740 um)
# 6 - vegetation red edge (0.783 um)
# 7 - vegetation red edge (0.865 um)
# 8 - SWIR (1.610 um)
# 9 - SWIR (2.190um)
extract_minipatches: False
minipatch_size: 32

# TRAINING PARAMETERS - PIX2PIX
# training_name: 'v0-128'
data_path: '/share_alpha_2/amandalucas/pix2pix/samples_patch_size_128/change_detection_true_two_classes_false/'
# data_path: '/share_alpha_2/amandalucas/pix2pix/Sentinel2/v0_256/samples_change_detection_true_two_classes_false'
training_steps: 350000
checkpoint_steps: 25000 # interval to save checkpoint
batch_size: 1 # The batch size of 1 produced better results for the U-Net in the original pix2pix experiment
lambda: 100 # weight on L1 term
gan_weight: 1.0 # weight on GAN term
image_width: 128
image_height: 128
output_channels: 3
#checkpoint_folder: '/share_alpha_2/amandalucas/pix2pix/samples_patch_size_128/change_detection_true_two_classes_false/pix2pix/27_10_2021_12_00_41/training_checkpoints' # used when running with --inference flag
# buffer_size: 100
ngf: 32 # number of generator filters in first conv layer
ndf: 32 # number of discriminator filters in first conv layer
lr: 0.0002 # initial learning rate for Adam
beta1: 0.5 # momentum for Adam
residual_generator: False # use residual blocks instead of skip connections
number_residuals: 3
synthetic_masks_path: 'trained_pix2pix_input_mode_1/'
binary_mask: False

# TRAINING PARAMETERS - UNET
synthetic_data_path: '/pix2pix/28_10_2021_01_11_16/synthetic_data_random/'
training_output_path: '/unet-results/'
input_channels: 6
epochs_unet: 100
batch_size_unet: 32
nb_filters: [32, 64, 128]
times: 5
patience_value: 10
